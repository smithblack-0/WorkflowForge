# Universal Declarative Prompting Language

## What is this?

This is the official specification for v1 of the 
UDPL prompting language. Linters and the ZCP compilers
should conform to this spec.

## Overview 

The Universal Declarative Prompting Language (UDPL)
is a TOML-based configuration format for defining
structured prompting sequences used in large language
model workflows. It allows developers to declaratively
specify sequences of prompt blocks, organize them into
zones (like [Prompt]...[Answer]), attach semantic tags,
and inject dynamic content using named placeholders.

Each UDPL file defines a set of named sequences, where
each sequence contains a series of prompt blocks. A
block consists of a predictable sequence of 
a multi-zone regions squeezed between special tokens,
and may specify up to all zones that were defined in the UDPL 
config section. This can be used to teacher-force the 
prompt.

Alternatively, by leaving the special tokens indicating
zone transitions outside the prompt string you may let the 
model generate this region instead. This allows 
fine-grained control over when prompting
ends and generation begins — entirely from the config file.

In addition to prompt structure, UDPL supports:

- Zone tagging, which enables selective extraction of
  generated outputs (e.g., for training, evaluation,
  or feedback)
- Placeholder resolution, using resource-backed
  dynamic fill-in at ZCP.
- Repeat and tagset patterns, for efficient contrastive
  or curriculum-style data generation

The outcome of parsing a valid UDPL file or folder is
a specification indicating what to feed, and in what
order, but lacking any indications of what to do when
flow control is encountered. This must later be
programmed in using SACS to produce an actual ZCP
IR graph. A dictionary of Zone Sequences is what is
ultimately constructed and returned when parsing.

While this language is intended to work with the
SUPS systems, others are encouraged to perform
pull requests to make their own extensions for
their particular use cases; we hope UDPL can become
an industry standard for better prompting configuration.



UDPL itself does not define control flow — it is consumed by downstream systems that run prompts linearly or with flow control. Its design supports both use cases equally, and serves as a flexible frontend for declaratively configuring prompt-based generation.


The Universal Declarative Preprocessing Language is a domain-specific language
intended to elegantly support sequential prompting workloads for NLP 
processes and also provide information which can subsequently be used to
extract relevant regions of text. 




The preprocessing mechanism for this generation of models is responsible
for elegantly resolving dynamic dependencies, alternating between 
generative and prompting states, and performing other relevant prompt
construction and generation actions.

This is accomplished using a zone-based config language that is designed
to feed a human-readable set of blocks into the generative stages of
the model. Each zone can have separate tags attached that then
may separately encode various tags that belong in those zone. Each 
block is assumed to go through a cycle of generating or being prompted
from all zones. Zones which are not fully defined by the user are instead
generated by the model, up to the generation limit.

Putting these together, we divided up the prompting and preprocessing
control system into two portions. T

- A configuration frontend consisting of a small DSL config
  language which lets you define, in order, the prompting sequence
  to give and define placeholders to be dynamically filled by 
  backend resources using the indicated arguments.
- A backend system consisting of objects we call 'resources'
  which are used to service these placeholder requests. The resources
  should be combined together into a service dictionary.

With this mechanism, we can define resources to consistently 
return the same string, as in a consitution overview, sample from
options, or even be a feedback resource which can be updated by
downstream logic using side effects, providing a flexible solution
which is not implementation dependent on the resources themselves.

## Terminology

Going forward it is important to keep the following
terms in mind:

- Zone: A region of tokens delimited by two special
  tokens defined in the config (e.g., a [Prompt] to
  [Reason] span). Zones are the unit of tagging and
  extraction. Every block is made up of one or more
  zones.

- Zone Edge Token: Special tokens such as [Prompt] or
  [Reason] used to indicate the edge of a zone.

- Block: A full prompt sequence consisting of one or
  more adjacent zones (e.g., [Prompt]...[Reason]...
  [Answer]...[EOS]). All zones in a block are resolved
  before the block completes. The model may take over
  any time all original prompt tokens are exhausted,
  at which point it completes the remaining zones.

- Prompt: A partially completed input string in a block
  that includes one or more defined zones. Prompts may
  include placeholder fields to be filled by resources
  during preprocessing.

- Tags: Labels applied to individual zones within a
  block to enable selective extraction of generated
  tokens for training, evaluation, or filtering.


## Config

All UDPL files must include a single [config] section.
If parsing a folder, one file in the folder must
contain this section. The config defines the set of
zone boundaries, required zones, valid tags, and
generation limits. These options are global and
govern how all sequences and blocks are interpreted.

A typical config looks like this:

```toml
[config]
zone_tokens = ["[Prompt]",
               "[Answer]", "[EOS]"]
required_tokens = ["[Prompt]", "[Reasoning]"]
valid_tags = ["Training", "Correct", "Incorrect1",
              "Incorrect2", "Incorrect3"]
default_max_token_length = 20000
sequences = ["setup", "loop", "solving", "concluding"]
control_token = "[Jump]"
escape_token = "[Escape]"
```

- `zone_tokens` defines the edge of each zone.
  Zones are the spans between each pair of tokens.
- `required_tokens` lists tokens that must be present
  in every blocks text feature as part of the prompt.
- `valid_tags` are the labels that can be attached
  to zones for later extraction.
- `default_max_token_length` sets the maximum token
  budget per zone, unless overridden in a block.
- `sequences` is the list of named sequences. Each
  corresponds to a section of blocks in the file.
- `control_token` is a special token used by the model
  to trigger flow control transitions.
- `escape_token` is a special token that disables a
  control transition when generated. This can skip
  zone advancement or flow control.

This configuration governs the interpretation of all
blocks defined in the file. Note you will see errors 
later on if you do not provide singular special tokens
in your tokenizer corresponding to this configuration.

If desired, additional entries can be parsed into 
you config and will be returned in a ZoneConfig
dataclass under the .misc feature. You may consider
putting hyperparameters and other features here.

## Blocks

### What Is a Block?

A block is a single structured unit within a UDPL
sequence. Each block defines a multi-zone prompt
template composed of one or more adjacent zones, such
as [Prompt]...[Answer]. Zones may be fully specified
(teacher-forced) or left incomplete to be generated by
the model.

Blocks are grouped under named sequences. Each block
must appear under a header of the form:

[[<sequence_name>]]

The sequence name must be one of the names listed in
the `[config]` section under `sequences`.

Each block is processed in the order it appears. All
zones in the block must eventually be resolved before
the next block begins, either by prompting or by model
generation.

### Block Fields

Each block must define a minimum of two fields:
- `text`: The prompt template containing zone tokens
- `tags`: A list of tag lists, one per zone span

Optional fields may also be included to control how
blocks are repeated, tagged, or filled dynamically.

---

#### Required Fields

- `text`: A multiline string containing the prompt.
  It must include any tokens listed in the
  `required_tokens` field of the config. These tokens
  mark zone **boundaries**, not zones themselves.
  Each zone is defined as the span between two
  adjacent zone tokens. Zones not included in the
  `text` will be generated by the model.

- `tags`: A list of sublists, one for each zone span.
  Each sublist contains zero or more tags to apply to
  the corresponding zone. The number of sublists must
  be exactly one less than the number of `zone_tokens`
  declared in the config. This is due again to the fact
  that zones are the text between the zone edges.

#### Optional Fields

- `repeats`: An integer. If present, the block will be
  repeated this number of times. All resource fills
  will be independently resampled for each repetition.
  The same tags are used for each repetition.

- `tagset`: A list of tag lists for each repetition.
  If provided, this replaces both `tags` and `repeats`.
  Each entry in the tagset is a full tag list (same
  structure as `tags`), and defines tags for one
  repetition of the block. This is used to produce
  multiple examples with the same text, but different 
  resource fills and tagging metadata.

- `max_gen_tokens`: An integer specifying the maximum
  number of tokens the model is allowed to generate
  per zone in this block. **Warning**: Some backends
  may enforce this limit even during teacher-forcing,
  which can cause the zone to be truncated. Use with
  care when manually specifying long zones.

- `[<sequence>.<placeholder>]`: One entry per
  placeholder used in `text`. Each must define the
  `name` of a resource to call, and may include
  `arguments` if needed. These interact with the 
  optional but recommended resource system.

### Text Rules

the `text` required field has certain flow control responsibilities that also need to be formally checked. In particular, it does not make sense to include a flow control token --- in our examples "[Jump]" but exact details depend on configuration --- in the text stream without escaping the jump first; otherwise, the TTFA will happily follow flow control due to teacher-forced prompts. This is checked for.

### Placeholders & Resource Bindings

Blocks may include placeholders in their `text` field
using `{placeholder_name}` syntax. These placeholders
are resolved at runtime using external resources.

Each placeholder must have a corresponding TOML table
entry under the same sequence. The structure is:

```toml 
[<sequence>.<placeholder_name>]
name = "resource_name"
arguments = { ... }      # optional
```

#### Basic Resource Example

This example uses a static resource named `principles`.

```toml
[[solving]]
text = """
[Prompt] Consider this: {philosophy}
[Answer]
"""
tags = [["Training"], ["Correct"]]

[solving.philosophy]
name = "principles"
```

#### Resource with Arguments

This example uses a sampler that returns multiple
points, controlled by `arguments`. This is useful
when you are sampling from consistituions of points,
for example.

```toml

[[solving]]
text = """
[Prompt] Consider: {feedback}
[Answer]
"""
tags = [["Training"], ["Feedback"]]

[solving.feedback]
name = "feedback_sampler"
arguments = { num_samples = 3 }
```

Each resource must return a string when invoked. If a
block is repeated, the resource is resampled each time.

#### Advanced: Resource Types

You may optionally specify a `type` field for a
placeholder binding. This allows extensions to UDPL
that change how resources are interpreted, but removes
the general cross-usage of resources from being shared
among all sequences to being a sequence speficific dynamic
dependendency.

One built-in extension is `type = "flow_control"`.
These resources inject structured values into the
prompt, such as loop bounds or flow-triggering tokens.

```toml
[[loop]]
text = """
[Prompt] Repeat {min} to {max} times. Emit the
[Escape] "[Jump]" token when you are ready to break.
Otherwise just say continue. 
[Answer]
"""
tags = [[], []]

[loop.min]
name = "min_value"
type = "flow_control"

[loop.max]
name = "max_value"
type = "flow_control"
```

**Note**: When `type` is used, the resource is bound
to the sequence in which it appears. All such
resources must be resolved before that sequence can
be compiled. This allows more specific or dynamic
control, but also means a compiled sequence must
provide those resources. This is not true when 
type is not used, in which case this resource
can be resolved from the general pool.


### Repeats & Tagsets

Blocks can be repeated using either the `repeats`
field or the `tagset` field. These mechanisms allow
you to generate multiple examples from a single
prompt structure.

#### Using `repeats` with `tags`

The `repeats` field is an integer. It causes the block
to be repeated N times using the same `text` and `tags`.

Each repetition will independently resample any
resources used in the block.

```toml
[[training]]
text = """
[Prompt] Analyze the following principle: {point}
[Answer]
"""
tags = [["Training"], ["Correct"]]
repeats = 3

[training.point]
name = "principle_sampler"
arguments = { num_samples = 1 }
```

This produces three blocks with the same tagging and
text structure, but different sampled content from
the resource.

#### Using `tagset`

The `tagset` field defines a list of tag lists, one
per repetition. It replaces both `tags` and `repeats`.

Each entry in the tagset must match the structure of
a valid `tags` field: one list per zone.

```toml
[[contrast]]
text = """
[Prompt] Create a flawed argument based on: {claim}
[Answer]
"""
tagset = [
  [[], ["Incorrect1"]],
  [[], ["Incorrect2"]]
]

[contrast.claim]
name = "claim_sampler"
arguments = { num_samples = 1 }
```

This creates two blocks with the same structure but
different tagging metadata.

#### When do I use tags vs tagset?

Exactly one of `tags` or `tagset` must be used per
block. If `tagset` is used, `repeats` is not allowed. 
Use `repeats` to create multiple examples with the
same tags and varying content. Use `tagset` to apply
different tag sets to each repetition.



## Blocks

After a config, a sequence of [[blocks]] are defined. 
These each will be returned in the sequence they are
defined, and are a complete collection of all intentions
for this region. They contain

- A required `text` field: a string representing the
  full prompt, optionally containing `{formatting_fill}`
  placeholders. This must use any required tokens in 
  the string.
- A required `tags` field: a list of  sublists,
  specifying tags for each of the three logical spans:
  `[Prompt]`, `[Reasoning]`, and `[Answer]`.
  Each sublist must be present, though it may be empty.
  This is one minus the number of zone tokens in length
- If placeholders are defined then `[blocks.<fill>]`
  entries corresponding to each `{fill}` in `text`: 
  these define how to invoke callbacks to 
  resolve each placeholder using external resources.

#### Simple Examples

An example of a simple valid block compatible with the
earlier config would be as follows. Notice how all four
prompt tokens are provided. This also means all three
zones are specified, and as such the entire example
would be teacher-forced. Notice as well we tag each of the 
three zones as train.

```toml
[[blocks]]
text ="""
[Prompt] 
Think about what it would take to generate good philosophical scenarios. Just tell me when you are done
[Reasoning]
Okay, it looks like the user wants...(omitted)
[Answer]
I am done
[EOS]
"""
tags = [["Train"],["Train"],["Train"]]
```

The model can be made to generate content by not providing
all these zones. Generation will pick up immediately after
the last token, and the generation logic would then be expected
to advance to the next zone when the next zone boundary token
is emitted. If this followed the last block, it would have then
been loaded.

```toml
[[blocks]]
text ="""
[Prompt] 
Now, actually make an interesting philosophical scenario
[Reasoning]
Okay, let me think this out.
"""
tags = [[],[],["Answer"]]
```

Notice that the model takes over right after the end of
the string. Additionally, since we are only tagging the 
answer we can later very easily only extract those tokens.

### Per-Zone Token Limits

Each block may optionally define a `max_gen_tokens`
field. This sets a hard limit on the number of tokens
that may be emitted per zone — whether the zone is
teacher-forced or generated.

The limit applies **per zone**, not per block.

Once the limit is reached, the default backend advances to 
the next zone, even if more prompt tokens remain or
no zone boundary has been emitted by the model.

```toml
[[revision]]
text = """
[Prompt] Revise your prior answer using this hint:
{hint}
[Answer]
"""
tags = [["Training"], ["Training"]]
max_gen_tokens = 128

[revision.hint]
name = "hint_sampler"
arguments = { num_samples = 1 }
```

**Warning:** The default backend enforces this limit
strictly, even for teacher-forced zones. If a zone's
content exceeds `max_gen_tokens`, the system will
truncate the zone and move on. This applies even when
the zone is provided entirely by the prompt, and is 
a trade-off made to make the backend simple enough to
exist purely using vectorized tensor logic.

Other backends may handle this differently, or upgraded
versions may eventually include memory protection.

To avoid unintended cutoff, either omit the field
or ensure the limit exceeds the longest expected zone.
It is intended that the compiler will eventually emit an 
error when this happens. 

# Parsing Details

When invoked, one ends up with a Config, and a Sequences dictionary. The sequences dictionary, as you would expect, maps to the individual sequences. These sequences are arrays of Zones, which are initial primitives stating, in untokenized form, what they need, when, and how. They will eventually become ZCP naturally.

# Using the library.

## Straightforward example.

### UDPL file.

Suppose we have a straightforward UDPL file. This is 
a simple straightthrough self-play example.

```toml
[config]
zone_tokens = ["[Prompt]", "[Answer]", "[EOS]"]
required_tokens = ["[Prompt]", "[Answer]"]
valid_tags = ["Training", "Final"]
default_max_token_length = 20000
sequences = ["blocks"]
control_token = "[Jump]"
escape_token = "[Escape]"

[[blocks]]
text="""[Prompt]Consider and resolve a philosophical dilemma
according to the following principles: {placeholder} 
[Answer]
Okay, I should think this through. 
"""
tags=[["Training"], []]
[blocks.placeholder]
name = "constitution_overview"

[[blocks]]
text="""[Prompt]Revise your previous answer after
considering the following additional details.
Make sure to also pretend you are directly answering
the prior response: {details}
[Answer]
Okay, I should begin by thinking through the new point, then
consider if I should revise my answer. Then I give my final 
answer.
"""
tags= [[], []]
[blocks.details]
name = "constitution_details"
arguments = {"num_samples" : 3}
repeats = 3

[[blocks]]
text = """[Prompt]
Consider your reflections so far. State a perfect answer
as though you jumped straight to the right answer.
[Answer]"""
tags = [[], ["Final"]]
```

This runs a simple philosophical reflection exercises that has the model produce a more refined
answer at the end; a union between the training and correct tags can refine this for synthetic training data
purposes. The reflection step runs three times.



### Code 

In python, we will manually define a few principles we wish to follow using the backend resources.
Then we will parse the UDPL.

```python
from CE import sups
from mycustomcode import parse_constitutions

# User specifications for the philosophy.
my_philosophy_overview= """
... whatever
"""
my_details = ["...whatever", "...whatever", ...]

# Create resources
resources = {}
resources["constitution_overview"] = sups.StaticStringResource(my_philosophy_overview)
resources["constitution_details"] = sups.StringListSampler(my_details)

# Parse the UDPl. Sequences now contains a 'block' factory that makes a sequence factory
sequences, config, tag_converter = sups.parse_udpl_file('prompts.toml', resources)
```

If you are using the SACS flow control system, this could then be continued into a 
tagging and extraction program that compiles to the ZCP IR. This is straightforward

```python
program = sups.sacs_program(sequences, config)
program.run(sequence="blocks")
program.extract(name="synthetic_answer", tags=["Training", "Final"])
zcp = program.compile()
```

The result can then go through the backend manager, or into your own custom backend

```python
manager = sups.load_backend(zcp)
```


## Straightforward example.

### UDPL file.

Suppose we have a straightforward UDPL file. This is 
a simple straightthrough self-play example.

```toml
[config]
zone_tokens = ["[Prompt]", "[Answer]", "[EOS]"]
required_tokens = ["[Prompt]", "[Answer]"]
valid_tags = ["Training", "Final"]
default_max_token_length = 20000
sequences = ["blocks"]
control_token = "[Jump]"
escape_token = "[Escape]"

[[blocks]]
text="""[Prompt]Consider and resolve a philosophical dilemma
according to the following principles: {placeholder} 
[Answer]
Okay, I should think this through. 
"""
tags=[["Training"], []]
[blocks.placeholder]
name = "constitution_overview"

[[blocks]]
text="""[Prompt]Revise your previous answer after
considering the following additional details.
Make sure to also pretend you are directly answering
the prior response: {details}
[Answer]
Okay, I should begin by thinking through the new point, then
consider if I should revise my answer. Then I give my final 
answer.
"""
tags= [[], []]
[blocks.details]
name = "constitution_details"
arguments = {"num_samples" : 3}
repeats = 3

[[blocks]]
text = """[Prompt]
Consider your reflections so far. State a perfect answer
as though you jumped straight to the right answer.
[Answer]"""
tags = [[], ["Final"]]
```

This runs a simple philosophical reflection exercises that has the model produce a more refined
answer at the end; a union between the training and correct tags can refine this for synthetic training data
purposes. The reflection step runs three times.



### Code 

In python, we will manually define a few principles we wish to follow using the backend resources.
Then we will parse the UDPL.

```python
from CE import sups
from mycustomcode import parse_constitutions

# User specifications for the philosophy.
my_philosophy_overview= """
... whatever
"""
my_details = ["...whatever", "...whatever", ...]

# Create resources
resources = {}
resources["constitution_overview"] = sups.StaticStringResource(my_philosophy_overview)
resources["constitution_details"] = sups.StringListSampler(my_details)

# Parse the UDPl. Sequences now contains a 'block' factory that makes a sequence factory
sequences, config, tag_converter = sups.parse_udpl_file('prompts.toml', resources)
```

If you are using the SACS flow control system, this could then be continued into a 
tagging and extraction program that compiles to the ZCP IR. This is straightforward

```python
program = sups.sacs_program(sequences, config)
program.run(sequence="blocks")
program.extract(name="synthetic_answer", tags=["Training", "Final"])
factory = program.compile()
```

The result can then go through the backend manager, or into your own custom backend

```python
manager = sups.load_backend(zcp)
```

## Flow-controlled reflection example

Suppose we want to structure a model that reflects on a
philosophical scenario, revises its answer over a few
iterations, and finally decides when it's ready to stop.
This is a flow-controlled loop, with auditing on each
step and a final answer extracted for training.

The loop is model-controlled: the model is told that if
it is ready to stop, it should emit [Jump]. We use
[Escape][Jump] in the prompt to avoid triggering
flow control during teacher-forcing. The actual [Jump]
token must be emitted unescaped by the model to break
the loop.

### UDPL File


```toml
[config]
zone_tokens = ["[Prompt]", "[Answer]", "[EOS]"]
required_tokens = ["[Prompt]", "[Answer]"]
valid_tags = ["Audit", "Final"]
default_max_token_length = 20000
sequences = ["setup", "loop", "reflect", "conclude"]
control_token = "[Jump]"
escape_token = "[Escape]"

[[setup]]
text = """
[Prompt] Consider the following philosophical dilemma:
{scenario}
[Answer]
Understood.
"""
tags = [["Audit", "Final"], ["Audit"]]

[setup.scenario]
name = "scenario_sampler"
arguments = { num_samples = 1 }

[[reflect]]
text = """
[Prompt] Reason through the dilemma above and refine your answer.
[Answer]
"""
tags = [["Audit"], ["Audit"]]

[[loop]]
text = """
[Prompt] If you're ready to finalize, emit [Escape][Jump].
Otherwise, respond 'continue'. This loop runs between {min}
and {max} times.
[Answer]
"""
tags = [["Audit"], ["Audit"]]

[loop.min]
name = "min_loop"
type = "flow_control"

[loop.max]
name = "max_loop"
type = "flow_control"

[[conclude]]
text = """
[Prompt] Provide your best final answer to the original dilemma.
[Answer]
"""
tags = [[], ["Final"]]

```

This example teaches the model to reflect on a provided
scenario, consider whether it's done, and output a final
answer. The [Jump] token breaks the loop, but is only
acted on if it is not escaped. All intermediate steps are
tagged with Audit. The final answer is tagged final only on its 
answer zone as before.

### Code

In Python, we begin by loading the necessary resources.
These include the dilemma samplers and the loop
iteration bounds. We will assume we have a custom
function written this time.


```python

from CE import sups
from mycustomcode import parse_resources

resources = parse_resources("resources/")
```

Next, we parse the UDPL file. The returned sequences
contain a SequenceFactoryFactory for each declared
sequence. The config object contains the tag structure
and tokenizer bindings.

```python
sequences, config, tag_converter = sups.parse_udpl_file(
    "reflective_loop.toml",
    resources
)
```

We now construct a SACS program. The setup step loads
a scenario. The loop is a bounded flow-controlled
repeat that runs reflect. The loop exits when the
model emits [Jump], after the minimum number of
iterations has been met.

```python
program = sups.sacs_program(sequences, config, backend="default")

program.run("setup")
with program.while_loop("loop", min=2, max=6) as loop:
    loop.run("reflect")
program.run("conclude")
program.extract(name="training_data", tags=["Final"])
program.extract(name="audit_log", tags=["Audit"])

```
We compile the result into a deployment factory that will
deploy the backend injector. Under the hood, this is 
compiling into ZCP which is then handed off to the 
backend builder.

```python
factory = program.compile()
```

This allows the model to reason step by step, emit
tokens that trigger state transitions, and produce both
an auditable trace of its thinking and a final,
high-quality answer usable for training. Other backends, or 
even ZCP compilers, are of course possible.